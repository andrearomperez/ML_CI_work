{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Orthogonal Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import hdmpy\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive vs. Orthogonal Estimators\n",
    "\n",
    "In this notebook, we will explore the differences between the Naive and Orthogonal estimators for estimating the effect of a treatment $D$ on an outcome $Y$. We will generate histograms of the estimators' distributions for different numbers of trials $B$.\n",
    "\n",
    "## Model Specification\n",
    "\n",
    "Given the model:\n",
    "$$ Y = 10 \\cdot D + X \\cdot \\beta + \\epsilon $$\n",
    "\n",
    "where:\n",
    "- $D$ is the treatment variable.\n",
    "- $X$ is a matrix of covariates.\n",
    "- $\\beta$ is a vector of coefficients for the covariates.\n",
    "- $\\epsilon$ is the error term.\n",
    "\n",
    "The true effect of $D$ on $Y$ is $10$.\n",
    "\n",
    "Additionally, the covariates $X$ also explain the treatment $D$:\n",
    "$$ D = X \\cdot \\gamma + \\nu $$\n",
    "\n",
    "where $\\gamma$ is a vector of coefficients for the covariates and $\\nu$ is the error term.\n",
    "\n",
    "## Naive Estimator\n",
    "\n",
    "The Naive estimator is obtained by fitting a simple linear model of $Y$ on $D$, without adjusting for the covariates $X$. This can lead to biased estimates if $X$ contains confounding variables that are correlated with both $D$ and $Y$.\n",
    "\n",
    "### Mathematical Model\n",
    "\n",
    "$$ \\hat{Y}_{\\text{Naive}} = \\alpha + \\hat{\\beta}_{\\text{Naive}} D $$\n",
    "\n",
    "Here, $\\hat{\\beta}_{\\text{Naive}}$ is the estimated coefficient of $D$ from the Naive model.\n",
    "\n",
    "## Orthogonal Estimator\n",
    "\n",
    "The Orthogonal estimator is designed to adjust for the confounding variables in $X$. It does this by first regressing both $Y$ and $D$ on $X$ to obtain residuals, and then regressing the residuals of $Y$ on the residuals of $D$.\n",
    "\n",
    "### Mathematical Model\n",
    "\n",
    "1. Regress $Y$ on $X$ to obtain residuals:\n",
    "   $$ \\hat{Y} = \\alpha_Y + X \\cdot \\beta_Y + \\hat{\\epsilon}_Y $$\n",
    "   Residuals: $\\hat{\\epsilon}_Y = Y - \\hat{Y}$\n",
    "\n",
    "2. Regress $D$ on $X$ to obtain residuals:\n",
    "   $$ \\hat{D} = \\alpha_D + X \\cdot \\beta_D + \\hat{\\epsilon}_D $$\n",
    "   Residuals: $\\hat{\\epsilon}_D = D - \\hat{D}$\n",
    "\n",
    "3. Regress the residuals of $Y$ on the residuals of $D$:\n",
    "   $$ \\hat{\\epsilon}_Y = \\alpha + \\hat{\\beta}_{\\text{Orthogonal}} \\hat{\\epsilon}_D $$\n",
    "\n",
    "Here, $\\hat{\\beta}_{\\text{Orthogonal}}$ is the estimated coefficient of $\\hat{\\epsilon}_D$ from the Orthogonal model.\n",
    "\n",
    "## Implementation and Results\n",
    "\n",
    "We will run simulations for $B = 100$, $B = 1000$, and $B = 10000$ trials to observe the distribution of the Naive and Orthogonal estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "np.random.seed(0)\n",
    "B = 100\n",
    "Naive = np.zeros( B )\n",
    "Orthogonal = np.zeros( B )\n",
    "    n = 100\n",
    "    p = 100\n",
    "    beta = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "    gamma = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "\n",
    "    mean = 0\n",
    "    sd = 1\n",
    "    X = np.random.normal( mean , sd, n * p ).reshape( n, p )\n",
    "\n",
    "    D = ( X @ gamma ) + np.random.normal( mean , sd, n ).reshape( n, 1 )/4 # We reshape because in r when we sum a vecto with a matrix it sum by column\n",
    "    \n",
    "    # DGP \n",
    "    Y = D + ( X @ beta ) + np.random.normal( mean , sd, n ).reshape( n, 1 )\n",
    "\n",
    "    # single selection method\n",
    "    r_lasso_estimation = hdmpy.rlasso( np.concatenate( ( D , X ) , axis  =  1 ) , Y , post = True ) # Regress main equation by lasso\n",
    "\n",
    "coef_array = r_lasso_estimation.est[ 'coefficients' ].iloc[ 2:, :].to_numpy()    # Get \"X\" coefficients \n",
    "r_lasso_estimation.est\n",
    "\n",
    "for i in range( 0, B ):\n",
    "    n = 100\n",
    "    p = 100\n",
    "    beta = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "    gamma = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "\n",
    "    mean = 0\n",
    "    sd = 1\n",
    "    X = np.random.normal( mean , sd, n * p ).reshape( n, p )\n",
    "\n",
    "    D = ( X @ gamma ) + np.random.normal( mean , sd, n ).reshape( n, 1 )/4 # We reshape because in r when we sum a vecto with a matrix it sum by column\n",
    "    \n",
    "    # DGP \n",
    "    Y = D + ( X @ beta ) + np.random.normal( mean , sd, n ).reshape( n, 1 )\n",
    "\n",
    "    # single selection method\n",
    "    r_lasso_estimation = hdmpy.rlasso( np.concatenate( ( D , X ) , axis  =  1 ) , Y , post = True ) # Regress main equation by lasso\n",
    "\n",
    "    coef_array = r_lasso_estimation.est[ 'coefficients' ].iloc[ 2:, :].to_numpy()    # Get \"X\" coefficients \n",
    "\n",
    "    SX_IDs = np.where( coef_array != 0 )[0]\n",
    "\n",
    "    # In case all X coefficients are zero, then regress Y on D\n",
    "    if sum(SX_IDs) == 0 : \n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant(D) ).fit().summary2().tables[1].round(3).iloc[ 1, 0 ] \n",
    "\n",
    "    # Otherwise, then regress Y on X and D (but only in the selected coefficients)\n",
    "    elif sum( SX_IDs ) > 0 :\n",
    "        X_D = np.concatenate( ( D, X[:, SX_IDs ] ) , axis = 1 )\n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant( X_D ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "\n",
    "    # In both cases we save D coefficient\n",
    "        \n",
    "    # Regress residuals. \n",
    "    resY = hdmpy.rlasso( X , Y , post = False ).est[ 'residuals' ]\n",
    "    resD = hdmpy.rlasso( X , D , post = False ).est[ 'residuals' ]\n",
    "    Orthogonal[ i ] = sm.OLS( resY , sm.add_constant( resD ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Orto_breaks = [-1.2, -1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2]\n",
    "Naive_breaks = [-0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharex= True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].hist( Orthogonal - 1 , range = (-2, 2), density = True , bins = Orto_breaks )\n",
    "axs[1].hist( Naive - 1, range = (-2, 2), density = True , bins = Naive_breaks )\n",
    "\n",
    "axs[0].title.set_text('Orthogonal')\n",
    "axs[1].title.set_text('Naive')\n",
    "\n",
    "axs[0].set_xlabel( 'Orhtogonal - True' )\n",
    "axs[1].set_xlabel( 'Naive - True' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "B = 1000\n",
    "Naive = np.zeros( B )\n",
    "Orthogonal = np.zeros( B )\n",
    "    n = 100\n",
    "    p = 100\n",
    "    beta = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "    gamma = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "\n",
    "    mean = 0\n",
    "    sd = 1\n",
    "    X = np.random.normal( mean , sd, n * p ).reshape( n, p )\n",
    "\n",
    "    D = ( X @ gamma ) + np.random.normal( mean , sd, n ).reshape( n, 1 )/4 # We reshape because in r when we sum a vecto with a matrix it sum by column\n",
    "    \n",
    "    # DGP \n",
    "    Y = D + ( X @ beta ) + np.random.normal( mean , sd, n ).reshape( n, 1 )\n",
    "\n",
    "    # single selection method\n",
    "    r_lasso_estimation = hdmpy.rlasso( np.concatenate( ( D , X ) , axis  =  1 ) , Y , post = True ) # Regress main equation by lasso\n",
    "\n",
    "coef_array = r_lasso_estimation.est[ 'coefficients' ].iloc[ 2:, :].to_numpy()    # Get \"X\" coefficients \n",
    "r_lasso_estimation.est\n",
    "\n",
    "for i in range( 0, B ):\n",
    "    n = 100\n",
    "    p = 100\n",
    "    beta = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "    gamma = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "\n",
    "    mean = 0\n",
    "    sd = 1\n",
    "    X = np.random.normal( mean , sd, n * p ).reshape( n, p )\n",
    "\n",
    "    D = ( X @ gamma ) + np.random.normal( mean , sd, n ).reshape( n, 1 )/4 # We reshape because in r when we sum a vecto with a matrix it sum by column\n",
    "    \n",
    "    # DGP \n",
    "    Y = D + ( X @ beta ) + np.random.normal( mean , sd, n ).reshape( n, 1 )\n",
    "\n",
    "    # single selection method\n",
    "    r_lasso_estimation = hdmpy.rlasso( np.concatenate( ( D , X ) , axis  =  1 ) , Y , post = True ) # Regress main equation by lasso\n",
    "\n",
    "    coef_array = r_lasso_estimation.est[ 'coefficients' ].iloc[ 2:, :].to_numpy()    # Get \"X\" coefficients \n",
    "\n",
    "    SX_IDs = np.where( coef_array != 0 )[0]\n",
    "\n",
    "    # In case all X coefficients are zero, then regress Y on D\n",
    "    if sum(SX_IDs) == 0 : \n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant(D) ).fit().summary2().tables[1].round(3).iloc[ 1, 0 ] \n",
    "\n",
    "    # Otherwise, then regress Y on X and D (but only in the selected coefficients)\n",
    "    elif sum( SX_IDs ) > 0 :\n",
    "        X_D = np.concatenate( ( D, X[:, SX_IDs ] ) , axis = 1 )\n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant( X_D ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "\n",
    "    # In both cases we save D coefficient\n",
    "        \n",
    "    # Regress residuals. \n",
    "    resY = hdmpy.rlasso( X , Y , post = False ).est[ 'residuals' ]\n",
    "    resD = hdmpy.rlasso( X , D , post = False ).est[ 'residuals' ]\n",
    "    Orthogonal[ i ] = sm.OLS( resY , sm.add_constant( resD ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Orto_breaks = [-1.2, -1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2]\n",
    "Naive_breaks = [-0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharex= True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].hist( Orthogonal - 1 , range = (-2, 2), density = True , bins = Orto_breaks )\n",
    "axs[1].hist( Naive - 1, range = (-2, 2), density = True , bins = Naive_breaks )\n",
    "\n",
    "axs[0].title.set_text('Orthogonal')\n",
    "axs[1].title.set_text('Naive')\n",
    "\n",
    "axs[0].set_xlabel( 'Orhtogonal - True' )\n",
    "axs[1].set_xlabel( 'Naive - True' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "B = 10000\n",
    "Naive = np.zeros( B )\n",
    "Orthogonal = np.zeros( B )\n",
    "    n = 100\n",
    "    p = 100\n",
    "    beta = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "    gamma = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "\n",
    "    mean = 0\n",
    "    sd = 1\n",
    "    X = np.random.normal( mean , sd, n * p ).reshape( n, p )\n",
    "\n",
    "    D = ( X @ gamma ) + np.random.normal( mean , sd, n ).reshape( n, 1 )/4 # We reshape because in r when we sum a vecto with a matrix it sum by column\n",
    "    \n",
    "    # DGP \n",
    "    Y = D + ( X @ beta ) + np.random.normal( mean , sd, n ).reshape( n, 1 )\n",
    "\n",
    "    # single selection method\n",
    "    r_lasso_estimation = hdmpy.rlasso( np.concatenate( ( D , X ) , axis  =  1 ) , Y , post = True ) # Regress main equation by lasso\n",
    "\n",
    "coef_array = r_lasso_estimation.est[ 'coefficients' ].iloc[ 2:, :].to_numpy()    # Get \"X\" coefficients \n",
    "r_lasso_estimation.est\n",
    "\n",
    "for i in range( 0, B ):\n",
    "    n = 100\n",
    "    p = 100\n",
    "    beta = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "    gamma = ( 1 / (np.arange( 1, p + 1 ) ** 2 ) ).reshape( p , 1 )\n",
    "\n",
    "    mean = 0\n",
    "    sd = 1\n",
    "    X = np.random.normal( mean , sd, n * p ).reshape( n, p )\n",
    "\n",
    "    D = ( X @ gamma ) + np.random.normal( mean , sd, n ).reshape( n, 1 )/4 # We reshape because in r when we sum a vecto with a matrix it sum by column\n",
    "    \n",
    "    # DGP \n",
    "    Y = D + ( X @ beta ) + np.random.normal( mean , sd, n ).reshape( n, 1 )\n",
    "\n",
    "    # single selection method\n",
    "    r_lasso_estimation = hdmpy.rlasso( np.concatenate( ( D , X ) , axis  =  1 ) , Y , post = True ) # Regress main equation by lasso\n",
    "\n",
    "    coef_array = r_lasso_estimation.est[ 'coefficients' ].iloc[ 2:, :].to_numpy()    # Get \"X\" coefficients \n",
    "\n",
    "    SX_IDs = np.where( coef_array != 0 )[0]\n",
    "\n",
    "    # In case all X coefficients are zero, then regress Y on D\n",
    "    if sum(SX_IDs) == 0 : \n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant(D) ).fit().summary2().tables[1].round(3).iloc[ 1, 0 ] \n",
    "\n",
    "    # Otherwise, then regress Y on X and D (but only in the selected coefficients)\n",
    "    elif sum( SX_IDs ) > 0 :\n",
    "        X_D = np.concatenate( ( D, X[:, SX_IDs ] ) , axis = 1 )\n",
    "        Naive[ i ] = sm.OLS( Y , sm.add_constant( X_D ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "\n",
    "    # In both cases we save D coefficient\n",
    "        \n",
    "    # Regress residuals. \n",
    "    resY = hdmpy.rlasso( X , Y , post = False ).est[ 'residuals' ]\n",
    "    resD = hdmpy.rlasso( X , D , post = False ).est[ 'residuals' ]\n",
    "    Orthogonal[ i ] = sm.OLS( resY , sm.add_constant( resD ) ).fit().summary2().tables[1].round(3).iloc[ 1, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Orto_breaks = [-1.2, -1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2]\n",
    "Naive_breaks = [-0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharex= True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].hist( Orthogonal - 1 , range = (-2, 2), density = True , bins = Orto_breaks )\n",
    "axs[1].hist( Naive - 1, range = (-2, 2), density = True , bins = Naive_breaks )\n",
    "\n",
    "axs[0].title.set_text('Orthogonal')\n",
    "axs[1].title.set_text('Naive')\n",
    "\n",
    "axs[0].set_xlabel( 'Orhtogonal - True' )\n",
    "axs[1].set_xlabel( 'Naive - True' )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
